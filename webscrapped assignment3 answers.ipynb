{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ea96fb4",
   "metadata": {},
   "source": [
    "1. #Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceea438e",
   "metadata": {},
   "source": [
    "2. #In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b33053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbfbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4f5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/2643407586.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "#automating the browser using webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b848d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/500410494.py:2: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/500410494.py:5: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search_btn = driver.find_element_by_id(\"nav-search-submit-button\")\n"
     ]
    }
   ],
   "source": [
    "#searching for guitars\n",
    "search = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search.send_keys(\"Guitar\")\n",
    "\n",
    "search_btn = driver.find_element_by_id(\"nav-search-submit-button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64ae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10877f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/3410323651.py:2: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  for i in driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls= []\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b0fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:13: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  brand = driver.find_element_by_xpath(\"//td[@class='a-span9']/span\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:18: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  product = driver.find_element_by_id(\"productTitle\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:23: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  rate = driver.find_element_by_id(\"acrCustomerReviewText\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:28: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  total_price = driver.find_element_by_id(\"priceblock_dealprice\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:33: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  returns_total = driver.find_element_by_id(\"nav-line-1\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/4014652545.py:38: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  delivery_total = driver.find_element_by_id(\"mir-layout-DELIVERY_BLOCK-slot-SECONDARY_DELIVERY_MESSAGE_LARGE\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying loops\n",
    "\n",
    "brand_name= [] \n",
    "product_name= [] \n",
    "rating_total= [] \n",
    "price= []\n",
    "returns=[]\n",
    "delivery =[]\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath(\"//td[@class='a-span9']/span\")\n",
    "        brand_name.append(brand.text)\n",
    "    except:\n",
    "        brand_name.append('-')\n",
    "    try:\n",
    "        product = driver.find_element_by_id(\"productTitle\")\n",
    "        product_name.append(product.text)\n",
    "    except:\n",
    "        product_name.append('-')\n",
    "    try:\n",
    "        rate = driver.find_element_by_id(\"acrCustomerReviewText\")\n",
    "        rating_total.append('-')\n",
    "    except:\n",
    "        rating_total.append('-')\n",
    "    try:\n",
    "        total_price = driver.find_element_by_id(\"priceblock_dealprice\")\n",
    "        price.append(total_price.text)\n",
    "    except:\n",
    "        price.append('_')\n",
    "    try:\n",
    "        returns_total = driver.find_element_by_id(\"nav-line-1\")\n",
    "        returns.append(returns.text)\n",
    "    except:\n",
    "        returns.append('_')\n",
    "    try:\n",
    "        delivery_total = driver.find_element_by_id(\"mir-layout-DELIVERY_BLOCK-slot-SECONDARY_DELIVERY_MESSAGE_LARGE\")\n",
    "        delivery.append(delivery.text)\n",
    "    except:\n",
    "        delivery.append('_')\n",
    "len(brand_name)\n",
    "len(product_name)\n",
    "len(rating_total)\n",
    "len(price)\n",
    "len(returns)\n",
    "len(delivery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492c9f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Name of the product</th>\n",
       "      <th>number of ratings</th>\n",
       "      <th>price of the item</th>\n",
       "      <th>url of the page</th>\n",
       "      <th>returns</th>\n",
       "      <th>delivery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Guitar Acoustica Series, Electric Acou...</td>\n",
       "      <td>-</td>\n",
       "      <td>_</td>\n",
       "      <td>https://aax-eu.amazon.in/x/c/Qp7wn6MxbrE4AmH0E...</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Brand                                Name of the product  \\\n",
       "0  Kadence  Kadence Guitar Acoustica Series, Electric Acou...   \n",
       "\n",
       "  number of ratings price of the item  \\\n",
       "0                 -                 _   \n",
       "\n",
       "                                     url of the page returns delivery  \n",
       "0  https://aax-eu.amazon.in/x/c/Qp7wn6MxbrE4AmH0E...       _        _  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a dataframe\n",
    "\n",
    "product_page = pd.DataFrame({})\n",
    "\n",
    "product_page['Brand']= brand_name\n",
    "product_page['Name of the product']= product_name\n",
    "product_page['number of ratings']= rating_total\n",
    "product_page['price of the item']= price\n",
    "product_page['url of the page']= urls\n",
    "product_page['returns']= returns\n",
    "product_page['delivery']= delivery\n",
    "product_page  #displaying the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf088ae1",
   "metadata": {},
   "source": [
    "3) #Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4912da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5053c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1762679360.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n",
    "driver.get(\"https://images.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34731403",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13004/61096606.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/61096606.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    var Scraper = require('images-scraper');\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "var Scraper = require('images-scraper');\n",
    "\n",
    "const google = new Scraper({\n",
    "  puppeteer: {\n",
    "    headless: false,\n",
    "  },\n",
    "});\n",
    "\n",
    "(async () => {\n",
    "  const results = await google.scrape('banana', 10 , 'guitar', 10 , 'cars',10, 'flowers' , 10 ,'machine_learning',10 ,'cakes',10);\n",
    "  console.log('results', results);\n",
    "})();"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5812efd3",
   "metadata": {},
   "source": [
    "4)#Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bed1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of webdriver for google chrome\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using webdriver we'll now open the flipkart website in chrome\n",
    "url = 'https://flipkart.com'\n",
    "# We;ll use the get method of driver and pass in the URL\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_item):\n",
    "    '''\n",
    "    This function fetches the URL of the item that you want to search\n",
    "    '''\n",
    "    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n",
    "    # We'are replacing every space with '+' to adhere with the pattern \n",
    "    search_item = search_item.replace(\" \",\"+\")\n",
    "    return template.format(search_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the function is working properly or not\n",
    "url = get_url('mobile phones')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a253ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a soup object using driver.page_source to retreive the HTML text and then we'll use the default html parser to parse\n",
    "# the HTML.\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fa08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the 1st card from the complete list of cards\n",
    "item = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959567d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the model of the phone from the 1st card\n",
    "model = item.find('div',{'class':\"_4rR01T\"}).text\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the display option from the 1st card\n",
    "display = item.find_all('li')[1].text.strip()\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting camera options from the 1st card\n",
    "camera = item.find_all('li')[2].text.strip()\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the battery option from the 1st card\n",
    "battery = item.find_all('li')[3].text\n",
    "battery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba72d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the processir option from the 1st card\n",
    "processor = item.find_all('li')[4].text.strip()\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Warranty from the 1st card\n",
    "warranty = item.find_all('li')[-1].text.strip()\n",
    "warranty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting price of the model from the 1st card\n",
    "price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ee7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Libraries\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "def get_url(search_item):\n",
    "    '''\n",
    "    This function fetches the URL of the item that you want to search\n",
    "    '''\n",
    "    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n",
    "    search_item = search_item.replace(\" \",\"+\")\n",
    "    # Add term query to URL\n",
    "    url = template.format(search_item)\n",
    "    # Add term query placeholder\n",
    "    url += '&page{}'\n",
    "    return url\n",
    "\n",
    "def extract_phone_model_info(item):\n",
    "    \"\"\"\n",
    "    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n",
    "    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n",
    "    \"\"\"\n",
    "    # Extracting the model of the phone from the 1st card\n",
    "    model = item.find('div',{'class':\"_4rR01T\"}).text\n",
    "    # Extracting Stars from 1st card\n",
    "    star = item.find('div',{'class':\"_3LWZlK\"}).text\n",
    "    # Extracting Number of Ratings from 1st card\n",
    "    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n",
    "    # Extracting Number of Reviews from 1st card\n",
    "    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n",
    "    # Extracting RAM from the 1st card\n",
    "    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n",
    "    # Extracting Storage/ROM from 1st card\n",
    "    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n",
    "    # Extracting whether there is an option of expanding the storage or not\n",
    "    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n",
    "    # Extracting the display option from the 1st card\n",
    "    display = item.find_all('li')[1].text.strip()\n",
    "    # Extracting camera options from the 1st card\n",
    "    camera = item.find_all('li')[2].text.strip()\n",
    "    # Extracting the battery option from the 1st card\n",
    "    battery = item.find_all('li')[3].text\n",
    "    # Extracting the processir option from the 1st card\n",
    "    processor = item.find_all('li')[4].text.strip()\n",
    "    # Extracting Warranty from the 1st card\n",
    "    warranty = item.find_all('li')[-1].text.strip()\n",
    "    # Extracting price of the model from the 1st card\n",
    "    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n",
    "    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n",
    "    return result\n",
    "\n",
    "def main(search_item):\n",
    "    '''\n",
    "    This function will create a dataframe for all the details that we are fetching from all the multiple pages\n",
    "    '''\n",
    "    driver = webdriver.Chrome()\n",
    "    records = []\n",
    "    url = get_url(search_item)\n",
    "    for page in range(1,464):\n",
    "        driver.get(url.format(page))\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        results = soup.find_all('a',{'class':\"_1fQZEK\"})\n",
    "        for item in results:\n",
    "            records.append(extract_phone_model_info(item))\n",
    "    driver.close()\n",
    "    # Saving the data into a csv file\n",
    "    with open('Flipkart_results.csv','w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model','Stars','Num_of_Ratings','Reviews','Ram','Storage','Expandable',\n",
    "                        'Display','Camera','Battery','Processor','Warranty','Price'])\n",
    "        writer.writerow(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now putting all the information from all the cards/phone models and putting them into a list\n",
    "records_list = []\n",
    "results = soup.find_all('a',{'class':\"_1fQZEK\"})\n",
    "for item in results:\n",
    "    records_list.append(extract_phone_model_info(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451bec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(records_list,columns=['model',\"star\",\"num_ratings\"\n",
    "   ,\"reviews\",'ram',\"storage\",\"expandable\",\"display\",\"camera\",\"battery\",\"processor\",\"warranty\",\"price\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "214a5103",
   "metadata": {},
   "source": [
    "5)#Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a76d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from selenium import webdriver\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# assign url in the webdriver object\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.co.in/maps/@10.8091781,78.2885026,7z\")\n",
    "sleep(2)\n",
    "\n",
    "\n",
    "# search locations\n",
    "def searchplace():\n",
    "\tPlace = driver.find_element_by_class_name(\"tactile-searchbox-input\")\n",
    "\tPlace.send_keys(\"Tiruchirappalli\")\n",
    "    \n",
    "\tSubmit = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button\")\n",
    "\tSubmit.click()\n",
    "\n",
    "searchplace()\n",
    "\n",
    "\n",
    "# get directions\n",
    "def directions():\n",
    "\tsleep(10)\n",
    "\tdirections = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div/button\")\n",
    "\tdirections.click()\n",
    "\n",
    "directions()\n",
    "\n",
    "\n",
    "# find place\n",
    "def find():\n",
    "\tsleep(6)\n",
    "\tfind = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/div/div/input\")\n",
    "\tfind.send_keys(\"Tirunelveli\")\n",
    "\tsleep(2)\n",
    "\tsearch = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/button[1]\")\n",
    "\tsearch.click()\n",
    "\n",
    "find()\n",
    "\n",
    "\n",
    "# get transportation details\n",
    "def kilometers():\n",
    "\tsleep(5)\n",
    "\tTotalkilometers = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[2]/div\")\n",
    "\tprint(\"Total Kilometers:\", Totalkilometers.text)\n",
    "\tsleep(5)\n",
    "\tBus = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[1]/span[1]\")\n",
    "\tprint(\"Bus Travel:\", Bus.text)\n",
    "\tsleep(7)\n",
    "\tTrain = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[2]/div[1]/div[2]/div[1]/div\")\n",
    "\tprint(\"Train Travel:\", Train.text)\n",
    "\tsleep(7)\n",
    "\n",
    "kilometers()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b07474c",
   "metadata": {},
   "source": [
    "6)#Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "const axios = require('axios')\n",
    "const cheerio = require('cheerio')\n",
    "const Crawler = require('simplecrawler')\n",
    "\n",
    "const HTMLParser = require('./HTMLParser')\n",
    "\n",
    "const TRAK_URLS = [\n",
    "  'http://trak.in/india-startup-funding-investment-2021/',\n",
    "  'http://trak.in/india-startup-funding-investment-2015/january-2021/',\n",
    "  'http://trak.in/india-startup-funding-investment-2015/february-2021/',\n",
    "  'http://trak.in/india-startup-funding-investment-2015/indian-startup-funding-investment-chart-march-2021/']\n",
    "\n",
    "function getUrls () {\n",
    "  return TRAK_URLS\n",
    "}\n",
    "\n",
    "async function scrape (urls) {\n",
    "  urls = urls || getUrls()\n",
    "  var rawData = []\n",
    "  try {\n",
    "    rawData = await axios.all(urls.map(axios.get))\n",
    "  } catch (e) {\n",
    "    console.error(e)\n",
    "  }\n",
    "  return rawData.reduce((a, b) => a.concat(b))\n",
    "}\n",
    "\n",
    "function crawl (url) {\n",
    "  return new Promise(function (resolve, reject) {\n",
    "    var crawler = new Crawler(url)\n",
    "    var results = []\n",
    "    crawler.on('fetchcomplete', function (queueItem, responseBuffer, response) {\n",
    "      var htmlParser = new HTMLParser(responseBuffer.toString('utf8'))\n",
    "      results.push(htmlParser.parse())\n",
    "    })\n",
    "    crawler.on('complete', function (queueItem, resources) {\n",
    "      console.log('Discovery Complete for: ' + url)\n",
    "      var result = results.reduce((acc, val) => {\n",
    "        return [...acc, ...val]\n",
    "      }, [])\n",
    "      console.log('Results', result.length)\n",
    "      resolve(result)\n",
    "    })\n",
    "    crawler.maxDepth = 2\n",
    "    crawler.discoverResources = function (buffer, queueItem) {\n",
    "      var $ = cheerio.load(buffer.toString('utf8'))\n",
    "\n",
    "      return $('a[href]')\n",
    "                  .filter(function () {\n",
    "                    var title = $(this).attr('title')\n",
    "                    return title && title.includes('Funding Data')\n",
    "                  })\n",
    "                  .map(function () {\n",
    "                    return $(this).attr('href')\n",
    "                  })\n",
    "                  .get()\n",
    "    }\n",
    "    crawler.start()\n",
    "  })\n",
    "}\n",
    "module.exports = {\n",
    "  scrape,\n",
    "  crawl,\n",
    "  HTMLParser,\n",
    "  getUrls\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e65529f",
   "metadata": {},
   "source": [
    "7)#Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9606fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b104757",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.digit.in/search/?keyword=best%20gaming%20laptops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen(url)\n",
    "page_html = page.read()\n",
    "page.close()\n",
    "page_soup = BeautifulSoup(page_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = page_soup.findAll(\"div\",class_=\"searchPage\")\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BeautifulSoup.prettify(containers[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb75e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = containers[4]\n",
    "prod_name = container.div.img[\"data-src\"]\n",
    "print(prod_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_price = container.findAll(\"div\",class_=\"common_container\")\n",
    "print(original_price[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f262d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_ratings = container.findAll(\"div\", class_=\"overall_rating\")\n",
    "print(prod_ratings[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = container.findAll(\"div\",class_= \"left_cont\")\n",
    "print(reviews[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a54a26d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'containers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13004/2501891989.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcontainer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontainers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mproduct_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"heading head-gaevent active\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprod_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moriginal_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"common_container\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'containers' is not defined"
     ]
    }
   ],
   "source": [
    "for container in containers[:5]:\n",
    "    product_name = container.findAll(\"h2\",class_=\"heading head-gaevent active\")    \n",
    "    prod_name = product_name[0].text.strip()\n",
    "    \n",
    "    original_price = container.findAll(\"div\",class_=\"common_container\")\n",
    "    original = original_price[0].text.strip()\n",
    "    \n",
    "    rating_container = container.findAll(\"div\", class_=\"overall_rating\")\n",
    "    prod_rating = rating_container[0].text.strip()\n",
    "    \n",
    "    reviews_container = container.findAll(\"div\",class_= \"left_cont\")\n",
    "    reviews_rating = reviews_container[0].text\n",
    "    \n",
    "    print(\"\\033[1mProduct Name: \\n\"+ '\\033[0m'+ str(prod_name), \"\\n\"),\n",
    "    print(\"\\033[1mOriginal Price: \\n\"+ '\\033[0m'+ str(original), \"\\n\"),\n",
    "    print(\"\\033[1mRatings: \\n\"+ '\\033[0m'+ prod_rating, \"\\n\"),\n",
    "    print(\"\\033[1mNumber of Reviews: \\n\"+ '\\033[0m'+ reviews_rating, \"\\n\"),\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac87cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8)Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c18a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:13: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Rank = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"rank\"]')))\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:14: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Name =list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"name\"]')))\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:15: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Net_Worth = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"networth\"]')))\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:19: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Age = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[6]')))\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:20: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Source = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[7]')))\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/1584729508.py:21: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  Country = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[8]')))\n"
     ]
    }
   ],
   "source": [
    "#import selenium package\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import csv\n",
    "import re\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\hamsa\\Desktop\\datascience project\\chrome webdriver\\chromedriver.exe\")\n",
    "driver.get(\"https://www.forbes.com/billionaires/list/50/#version:realtime\")\n",
    "\n",
    "csv_file = open('reviews.csv', 'w', encoding='utf-8')\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "Rank = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"rank\"]')))\n",
    "Name =list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"name\"]')))\n",
    "Net_Worth = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]/td[@class = \"networth\"]')))\n",
    "wait_button = WebDriverWait(driver, 10)\n",
    "\n",
    "\n",
    "Age = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[6]')))\n",
    "Source = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[7]')))\n",
    "Country = list(map(lambda x: x.text, driver.find_elements_by_xpath('//tbody/*[@class=\"data\"]//td[8]')))\n",
    "wait_button = WebDriverWait(driver, 10)\n",
    "\n",
    "Rank = [(re.sub('[\\n , . - #]+',\"\", i)) for i in Rank]\n",
    "Name = [(re.sub('[\\n,.-]+',\"\", i)) for i in Name]\n",
    "Net_Worth = [(re.sub('[\\n , - # B $]+',\"\", i)) for i in Net_Worth]\n",
    "Age = [(re.sub('[\\n,.-]', '', i)) for i in Age]\n",
    "Source = [(re.sub('[\\n,.-]', '', i)) for i in Source]\n",
    "Country = [(re.sub('[\\n,.-]', '', i)) for i in Country]\n",
    "\n",
    "data_dict = zip(Rank, Name, Net_Worth, Age, Source, Country)\n",
    "\n",
    "writer.writerows(data_dict)\n",
    "csv_file.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563f7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9)Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8620be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4330bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\hamsa\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n",
      "C:\\Users\\hamsa\\AppData\\Local\\Temp/ipykernel_13004/429547786.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "link = input(\"https://www.youtube.com/watch?v=tFX2UvkQj44&list=RDtFX2UvkQj44&start_radio=1\")\n",
    "driver.get(link)\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "title = driver.find_element_by_xpath('//*[@id=\"container\"]/h1/yt-formatted-string').text\n",
    "print(\"Video Title: \" + title)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "comment_section = driver.find_element_by_xpath('//*[@id=\"comments\"]')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", comment_section)\n",
    "time.sleep(7)\n",
    "\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\") #driver.execute_script() is used to run a java script in selenium\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "username_list = []\n",
    "comment_list = []\n",
    "name_elems=driver.find_elements_by_xpath('//*[@id=\"author-text\"]')\n",
    "comment_elems = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "num_of_names = len(name_elems)\n",
    "for i in range(num_of_names):\n",
    "    username = name_elems[i].text    # .replace(\",\", \"|\")\n",
    "    comment = comment_elems[i].text    # .replace(\",\", \"|\")\n",
    "    username_list.append(username)\n",
    "    comment_list.append(comment)\n",
    "        \n",
    "    print(username + \": \" + comment) # comment.translate({ord(i):None for i in '' if i not in string.printable})\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba983073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10)Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "url = 'https://www.hostelworld.com/hostels/london'\n",
    "response = get(url)\n",
    "\n",
    "# create soup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# creating individual containers, on each one there's information about one hostel.\n",
    "holstel_containers= soup.findAll(class_= 'fabresult rounded clearfix hwta-property')\n",
    "\n",
    "# how many hostels on the page\n",
    "len(holstel_containers)\n",
    "\n",
    "first_hostel = holstel_containers[0]\n",
    "\n",
    "# Hostel name\n",
    "first_hostel.h2.a.text\n",
    "print(first_hostel.prettify())\n",
    "# first, create the empty lists\n",
    "hostel_names= []\n",
    "hostel_links= []\n",
    "hostel_distance= []\n",
    "hostel_ratings= []\n",
    "hostel_reviews= []\n",
    "hostel_prices= []\n",
    "\n",
    "for page in np.arange(1,4): # to iterate over the pages and create the conteiners\n",
    "  url = 'https://www.hostelworld.com/hostels/london?page=' + str(page)\n",
    "  response = get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  holstel_containers= soup.findAll(class_= 'fabresult rounded clearfix hwta-property')\n",
    "\n",
    "  for item in range(len(holstel_containers)): # to iterate over the results on each page\n",
    "    hostel_names.append(holstel_containers[item].h2.a.text)\n",
    "    hostel_links.append(holstel_containers[item].h2.a.get('href'))\n",
    "    hostel_distance.append(holstel_containers[item].find(class_= \"addressline\").text[12:18].replace('k','').replace('m','').strip())\n",
    "    hostel_ratings.append(holstel_containers[item].find(class_='hwta-rating-score').text.replace('\\n', '').strip())\n",
    "    hostel_reviews.append(holstel_containers[item].find(class_=\"hwta-rating-counter\").text.replace('\\n', '').strip())\n",
    "    hostel_prices.append(holstel_containers[item].find(class_= \"price\").text.replace('\\n', '').strip()[3:])                          \n",
    "  time.sleep(2) # this is used to not push too hard on the website\n",
    "\n",
    "   hw_london = pd.DataFrame({\n",
    "    'hostel_name': hostel_names,\n",
    "    'distance_centre_km': hostel_distance,\n",
    "    'average_rating': hostel_ratings,\n",
    "    'number_reviews': hostel_reviews,\n",
    "    'average_price_usd': hostel_prices,\n",
    "    'hw_link': hostel_links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d6be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
